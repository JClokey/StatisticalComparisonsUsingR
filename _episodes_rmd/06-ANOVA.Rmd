---
title: "Testing For More Than Two Samples"
teaching: 45
exercises: 10
questions:
- "Are the group means different among three or more groups?"
objectives:
- "Identify situations needing multiple sample tests and choose the correct test 
for the type of data"
- "Perform one and two-way ANOVA testing"
- "Recognise interaction effects in multiple-category testing"
- "Interpret test results and apply post hoc testing"
keypoints:
- ""
output: html_document
---

Episode topics
Decision tree – ANOVA, Kruskal-Wallis, Friedman
ANOVA one-way: like T-test but with 3 or more groups
Post-hoc testing
Correction of multiple testing effects
ANOVA two-way: like T-test but with 2 or more categories
Interaction effects and analysis
Learning outcomes
Identify situations needing multiple sample tests
Choose the correct test for the type of data
Perform one and two-way ANOVA testing
Recognise interaction effects in multiple-category testing
Interpret test results and apply post hoc testing






```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
gallstones <- read.csv("data/gallstones.csv")
library(ggplot2)
library(gmodels)
library(PMCMRplus)
```

## Comparison of multiple groups

The T-test, Mann-Whitney Test and others discussed earlier are designed to 
identify differences in the means or medians of two groups. When working with 
data that is in three or more groups, where we are testing if there is a 
difference between any one of those groups with the others, we need to use other
tests. As with two-sample testing, the appropriate test is determined in large
part by whether the data in each group is normally distributed, and whether the
data is paired, as outlined in the figure below.

![RStudio layout](../fig/06-fig1.png)

> ## Challenge 1
>
> Based on what you have learned previously in this workshop, how can we best
> determine whether the data in each sample is normally distributed
> > ## Solution to Challenge 1
> > 
> > We can use the `shapiro.test` function to test for normality - or rather, to 
> > test the alternative hypothesis that the data is not normally distributed. 
> > Use the `by` function to test all categories in one command: 
> > ```{r eval = FALSE}
> > by(data$measurement, data$category, shapiro.test)
> > ```
> > Remember, as with the two sample tests, if any one group is not normally
> > distributed, the whole analysis must be performed with the relevant 
> > non-parametric test
> {: .solution}

## ANOVA Testing - One-way

The one-way ANOVA compares whether there is a difference in the mean values of 
three or more groups. It requires one continuous (and normally distributed) 
measurement variable, and one categorical variable (with three or more 
categories). 

Assumptions for the one-way ANOVA are:
* Independant samples
* Normal distribution in each group
* Homogeneity of variances

The null hypothesis for one-way ANOVA is that the means of all groups are equal;
the null hypothesis is that at least one of the means is different from the 
others.

H~0~: µ~1~ = µ~2~ = µ~3~ = ... = µ~k~
H~1~: µ~1~ ≠ µ~2~ OR µ~1~ ≠ µ~3~ OR µ~2~ ≠ µ~3~ ....

The ANOVA extension of the t-test is called the **F-test**, and is based around 
decomposing the total variation in the sample into the variability (sum of 
squares) within groups
and between groups















## Comparison of two sample groups

Earlier we discussed continuous data, and how to investigate relationships 
(correlations) between two continuous variables. In this section, we will learn 
how to identify whether a single continuous trait differs between two sample 
groups - a two sample test. Specifically, we will investigate whether there is a
statistically-significant difference between the distribution of that variable 
between the two groups. As an example, we will test whether male patients in our
gallstones study are taller than female patients.

> ## Discussion
>
> See if you can identify other examples where you might use a two sample groups
> comparison. Do you have any in your own research?
{: .discussion}

### Choosing the relevant test

As with testing for categorical variables, there are a range of different 
statistical analyses for two sample group comparisons; the appropriate one to 
use is determined by the nature of the dataset. There are two primary questions
we need to ask to identify the relevant test: are the two datasets 
normally-distributed, and are the data paired? The figure below summarises the 
choice of statistical test used for each of these cases.

![RStudio layout](../fig/05-fig1.png)

The first step is to determine whether the continuous variable in each group is
normally distributed. We've already learned about the `shapiro.test` function to 
test for normality, and can use that again in this situation.

The second decision is to identify whether the data is paired or not. Paired 
data is when the two groups are the same test samples but measured under 
different conditions (for example, a group of patients tested before and after 
treatment), unpaired is when the two groups are independent (for example, two 
separate groups of patients, one group treated and one untreated). 

There are a few further subtleties beyond this which we will come to in a 
moment, but these are the two major determining factors in choosing the correct
test.


> ## Challenge 1
>
> In our gallstones dataset, assume that BMI is normally distributed for
> patients with a recurrence of gallstones and not normal for those with no 
> recurrence. Which test would we use to investigate whether those two groups
> (with and without recurrence) had different BMIs?
> > ## Solution to Challenge 1
> > 
> > One data set is normally distributed, the other is not, so we choose the 
> > option for non-normally distributed data - the branch to the right (we can 
> > only answer yes to the first question if both datasets are normal). The data
> > is not paired - the patients with recurrence are a different group to those
> > without. In this case we would use the Mann-Whitney test.
> {: .solution}
{: .challenge}

## Two sample Student's T-test

If data is normally distributed for **both** groups, we will generally use the 
Student's T-test. This compares the means of two groups measured on the same 
continuous variable. Tests can be two-sided (testing whether the groups are not
equal) or one-sided (testing either whether the second group is greater than or
less that the first). As we discussed in the introduction, generally a two-sided
test is preferred unless there is a specific reason why a single-sided one is
justified.

H~0~: µ~1~ = µ~2~ | against | H~1~: µ~1~ ≠ µ~2~ (two-sided)
 | or | 
H~0~: µ~1~ <= µ~2~ | against | H~1~: µ~1~ > µ~2~ (greater)
 | or | 
H~0~: µ~1~ >= µ~2~ | against | H~1~: µ~1~ < µ~2~ (less)

If **equal variance**: Student's T-test  
If **unequal variance**: Welch's two-sample T-test  
If **data are paired**: Student's paired T-test  

> ## Tip
> The R `t.test` function combines all three of these tests, and defaults to 
> Welch's two-sample T-test. To perform a standard T-test, use the parameter 
> setting `var.equal = TRUE`, and for a paired T-test, use `paired = TRUE`. 
{: .callout}

## Two sample Mann-Whitney test

Unless **both** groups are normally distributed, use the Mann-Whitney test. This
is a non-parametric test analogous to the unpaired T-test, used when the 
_dependent_ variable is non-normally distributed

The Mann-Whitney test compares the medians of the two groups rather than the 
means, by considering the data as rank order values rather than absolute values.

> ## Tip
> The `wilcox.test` function in R defaults to unpaired data - effectively 
> returning the Mann-Whitney test instead. Carry out a paired Wilcox test with 
> the `paired = TRUE` argument
{: .callout}

## Two sample test example

Is there a difference in height between females and males in the gallstones 
dataset? 

Height: Continuous variable  
Gender: Categorical variable with two levels  
Null hypothesis: There is no difference in height between the groups  

_Step one - visualise the data_
We will start by reviewing the data using a boxplot to see if there is an 
indication of difference between the groups
```{r eval=FALSE}
plot(gallstones$Height ~ gallstones$Gender, 
     col=c('red','blue'),
     ylab = 'Height',
     xlab = 'Gender')
```
![RStudio layout](../fig/05-fig2.png)

Visually there certainly appears to be a difference. But is it statistically
significant?

_Step two - is the data normally distributed?_
```{r eval = FALSE}
par(mfrow=c(1,2))
hist(gallstones$Height[which(gallstones$Gender == 'F')], main = "Histogram of heights of females", xlab = "")
hist(gallstones$Height[which(gallstones$Gender == 'M')], main = "Histogram of heights of males", xlab = "")
```
![RStudio layout](../fig/05-fig3.png)

This doesn't look very normally-distributed, but we do have relatively few data
points. A more convincing way to determine this would be with the Shapiro-Wilks
test
```{r}
by(gallstones$Height, gallstones$Gender, shapiro.test)
```

Neither test gives a significant p-value, so in the absence of sufficient 
evidence to accept the alternative hypothesis of non-normality, we treat the
data as if it were normal; that is, we use a T-test 

_Step three - are variances equal?_
```{r}
by(gallstones$Height, gallstones$Gender, sd)
```

The standard deviations of the two groups (and hence the variances) don't seem 
to be equal, so we should use a Welch's two-sample T-test. This is the default
option for the `t.test` function anyway

_Step four - carry out a T-test_
```{r}
t.test(gallstones$Height ~ gallstones$Gender)
```

**Conclusion**: the p-value is significant so we can accept the alternative 
hypothesis and conclude that there is a difference in the mean height of males
and females in our dataset.


> ## Challenge 2
> 
> Using the gallstones dataset, test whether the gallstone diameter ("Diam") is
> different between patients who suffer a recurrence and those who do not.
> > ## Solution to Challenge 2
> > 
> > ```{r}
> > # Visualise data
> > plot(gallstones$Diam ~ gallstones$Rec, col = c("red","blue"),
> >      ylab = "Diameter",
> >      xlab = "Recurrence")
> > # Test whether data is normally distributecd
> > by(gallstones$Diam, gallstones$Rec, shapiro.test)
> > ```
> > Data is not normal for the recurrence group, and data is not paired - hence
> > Mann-Whitney test
> > ```{r}
> > # Use wilcox.test function which defaults to Mann-Whitney analysis
> > wilcox.test(gallstones$Diam ~ gallstones$Rec, exact=FALSE)
> > ```
> > The p-value is not significant, so we reject the alternative hypothesis that
> > there is a difference in gallstone size between the two groups.
> {: .solution}
{: .challenge}

## Group descriptions

If there is a significant difference between the two groups (or even if there
isn't) it is often useful to generate some summary statistics for each group. 
We can do this with the `by` command, which we've used already in this section, 
combined with summary functions

```{r}
# For normally distributed data, report the mean and standard deviation
by(gallstones$Height, gallstones$Gender, mean)
by(gallstones$Height, gallstones$Gender, sd)
```

```{r}
# For non-normally distributed data, report the median and inter-quartile range
by(gallstones$Diam, gallstones$Rec, median)
by(gallstones$Diam, gallstones$Rec, IQR)
```

## Paired samples
If data is paired, that is, it is the same samples under two different 
conditions, we can take advantage of that to carry out statistical tests with 
greater discriminatory power. That is because by using paired samples, we remove
a lot of the noise that can otherwise obscure our results. Paired data must have
the same number of results in each group, there must be a one-to-one relationship
between the groups (every sample that appears in one group must appear in the 
other), and the data must be the same sample order in each group.

Otherwise, paired sample analysis is performed in a similar way to unpaired 
analysis. The main difference is to add the `paired = TRUE` argument to the 
`t.test` or `wilcox.test` function.